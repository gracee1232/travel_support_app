# LLM API Configuration
# Supported: openai, mistral, openrouter, ollama
LLM_PROVIDER=ollama
LLM_API_KEY=your-api-key-here
LLM_BASE_URL=http://localhost:11434/v1
LLM_MODEL=mistral

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=true
